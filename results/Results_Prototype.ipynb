{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Prototype Notebook\n",
    "\n",
    "## Framing\n",
    "\n",
    "This notebook generates the linear probing and inter-class distance metrics for a trained Perceiver Autoencoder model (with or without predictive loss). \n",
    "\n",
    "## Design\n",
    " - Load the `validate` set from Kinetics 400. \n",
    "     - Parse the text file in `Multimodal_Foundation/datasets/Kinetics/frame_cnts_val.txt` to get the actual number of frames. \n",
    "     - Load some random segment from that number of frames (use random seed or store this somehow). \n",
    "     - Figure out how to make a `tf.Dataset` with labels from this. \n",
    " - Load a trained model, parse the data, generate latent representations of each video in the validation set. \n",
    " - Do a cheeky PCA on the data, take a look. \n",
    " - Figure out how the ViT-MAE paper performed linear probing. \n",
    " - Perform linear probing, compute top-5 accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 10:00:01.491344: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-21 10:00:01.633127: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-21 10:00:01.661182: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-21 10:00:02.293063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aman/miniconda3/lib/\n",
      "2022-12-21 10:00:02.293119: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aman/miniconda3/lib/\n",
      "2022-12-21 10:00:02.293123: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGICAL GPUs:  [LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-21 10:00:03.308089: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-21 10:00:04.750819: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-12-21 10:00:04.750861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21403 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2022-12-21 10:00:04.751739: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-12-21 10:00:04.751763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21405 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:68:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "## Import box\n",
    "import os \n",
    "import sys \n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "import math\n",
    "import argparse\n",
    "import datetime\n",
    "import pickle\n",
    "import pdb\n",
    "import time\n",
    "from packaging import version\n",
    "\n",
    "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\"\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "sys.path.append(\"../src\")       # Adding src folder to path\n",
    "sys.path.append(\"../src/utils\") # Adding utils folder to path\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "# import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import imageio\n",
    "\n",
    "import model\n",
    "import video_loader as vl\n",
    "import video_preprocess as vp\n",
    "import train_m2\n",
    "import parallel_video_loader as pvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "EXPERIMENT_FOLDER = ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Load Experiment Params\n",
    "\n",
    "To make sure we format the data properly, we need to know the contents of `data_format_args.pkl` from the relevant experiment. Let's load this dict here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load `data_format_args.pkl` !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Loading the Data (Kinetics-400 Validation Set)\n",
    "\n",
    "The relative path to the Kinetics 400 validation set is `../datasets/Kinetics/validate`. Within this folder, each of the 400 action classes has a subfolder containing videos of that class. \n",
    "\n",
    "There is also a text file called `frame_cnts_val.txt`. Each line pertains to one video file and is of the form `Easy: 411.0, hard: 301 -- validate/testifying/_yMeY9-Gi0k.mp4`. \"Easy\" refers to the number of frames as counted by the OpenCV default frame counting system, whereas \"hard\" refers to the number of frames as counted when actually reading in the videos. After that is the path of the video. The \"hard\" count for the number of frames should be used when sampling frames from the video as the \"easy\" count generally overestimates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_count_file_path = \"../datasets/Kinetics/validate/frame_cnts_val.txt\"\n",
    "dataset_path = \"../datasets/Kinetics\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a: Load `frame_cnts_val.txt`\n",
    "\n",
    "This should be loaded as a list of tuples of `(path, frame_count)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_counts(path_to_counts): \n",
    "    ... \n",
    "\n",
    "frame_counts = get_frame_counts(frame_count_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b: Load a List of Video Matrices (`numpy`)\n",
    "\n",
    "Use `parallel_video_loader`'s (`pvl`'s) `load_video_range(video_path, start_frame, num_frames, output_size)` function to load a random subset of the frames. For now, we can use the following parameters: \n",
    " - `num_frames`: 4\n",
    " - `output_size`: (120,180)\n",
    " - `start_frame`: 0 \n",
    "\n",
    "Use the `frame_counts` tuple to get the paths of each video and to make sure they have sufficiently many frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c: Convert/Process the List of Videos $\\to$ `tf.Tensor`\n",
    "\n",
    "Follow the procedure in [vp.get_videosets()](https://github.com/amanb2000/Multimodal_Foundation/blob/06bacc67f37c6dfb9de811ca78345fa312d9964d/src/utils/video_preprocess.py#L238) to convert the list of numpy matrices into patched/positionally encoded tensors. \n",
    "\n",
    "Optimally, we still want this to be a regular python `list` rather than a Tensorflow `Dataset`. This will make it easier to keep track of the labels in a parallel list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Load the Model \n",
    "\n",
    "From `EXPERIMENT_FOLDER` we need to instantiate and load the model weights. We can follow the procedure from [execute_experiment.py](https://github.com/amanb2000/Multimodal_Foundation/blob/06bacc67f37c6dfb9de811ca78345fa312d9964d/src/execute_experiment.py#L445)'s code that does this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Create Embeddings using the Model\n",
    "\n",
    "`NOTE`: We need to make sure our positional codes match the ones the model is expecting. Be sure to generate them, following `execute_experiment.py`'s method of doing that, and append the correct ones to the `FlatPatchSet`-equivalent Tensors. \n",
    "\n",
    "Store the embeddings in another parallel list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Some Cheeky PCA\n",
    "\n",
    "Also do the inter-cluster distances here :) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Linear Probing (Simple sklearn)\n",
    "\n",
    "Once done, make sure this is actually what they did in ViT-MAE..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMF",
   "language": "python",
   "name": "mmf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb424496d90d9eff7a93e13c98d7ddc1d5da3a5925d09c92a85f3c2f93884c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
