{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceiver Autoencoder\n",
    "\n",
    "In this notebook, I prototype the **full** version of the Perceiver autoencoder. This includes the training loop and any data management that needs to occur with this. Disk-memory-GPU management of data will not be performed, however. \n",
    "\n",
    "**Goals:**\n",
    "Fully adjustable + parameterizable Perceiver for Predictive Coding.\n",
    " - **Encoder**: Adjustable encoder shape, # re-exposures as data is compressed. \n",
    "\t - Query = current latent state; key-values = input byte array. \n",
    "\t \t - Re-exposures: query = current latent state estimate; key-values = input byte array.\n",
    "\t\t - **Option**: residual connections around TF blocks.  \n",
    "\t\t - **Option**: use different block types for each block after the first. \n",
    "\t - Let's avoid having a bunch of intermediate {#token, token dim} sizes between the original byte array and the final latent state. \n",
    "\t - The only adjustment should be the # re-exposures -- i.e., the number of different $\\mathbb{R}^{(M \\times D)} \\to \\mathbb{R}^{(N\\times C)}$ encoders there are that re-query the byte array using the current latent estimate. \n",
    " - **Latent-latent**: Number of distinct blocks, number of block repeats between new information exposure. \n",
    " - **Decoder**: Similar to encoder, queries = positional codes we want to reconstruct, key-values = latent matrix. \n",
    "\t - Avoid intermediate dimensionailties. \n",
    "\t - For repeated querying, do we use query = current reconstruction, key-value = latent matrix? \n",
    "\t \t - **Optional**: Can have residual connections, too!\n",
    "\t\t - **Optional**: use the same blocks for every step of re-exposure. \n",
    "\n",
    "\n",
    "## Pseudocode \n",
    "\n",
    "```python\n",
    "class Model: \n",
    "\tgoverning class vars: \n",
    "\t\tencoder = [several TF Blocks]\n",
    "\t\tnum_distinct_encoder = [int]\n",
    "\t\tnum_total_encoder = [int]\n",
    "\t\tresidual_encoder = [bool]\n",
    "\t\t\n",
    "\t\tlatent_evolver = [several TF Blocks]\n",
    "\t\tnum_distinct_latent = [int]\n",
    "\t\tnum_total_latent_cycles = [int] # if this is a 2-tuple: we randomly select some # of latent cycles in that range. \n",
    "\t\t\n",
    "\t\t\n",
    "\t\tdecoder = [several TF Blocks]\n",
    "\t\tnum_distinct_decoder = [int]\n",
    "\t\tnum_total_decoder = [int]\t\n",
    "\t\tresidual_decoder = [bool]\n",
    "\n",
    "\tclass state vars: \n",
    "\t\tlatent_state = [TF variable, learnable initial pos code]\n",
    "\n",
    "\tfunction encode(input_tokens): \n",
    "\t\t...\n",
    "\n",
    "\tfunction evolve_latent(): \n",
    "\t\t...\n",
    "\n",
    "\tfunction decode(positional_codes):\n",
    "\t\t...\n",
    "\n",
    "\n",
    "\tfunction test(new_datum): \n",
    "\t\t\"\"\" Basically just `call` but it won't incorporate the new datum into\n",
    "\t\tthe latent state. It will also add the test performance to the model's \n",
    "\t\t`test loss` records.  \n",
    "\t\t\"\"\"\n",
    "\t\t...\n",
    "\n",
    "\tfunction call(new_datum, return_latent=False):\n",
    "\t\t\"\"\" Given some new patches, we calculate the \"surprise\" \n",
    "\t\t\tthen incorporate the data into the latent state. \n",
    "\n",
    "\t\t\tWe finally return the surprise value, just for metric tracking. \n",
    "\t\t\"\"\"\n",
    "\t\t# Computing surprise\t\t\n",
    "\t\tpredicted_input = decode(new_datum.positional_codes)\n",
    "\t\tnew_loss = loss(predicted_input, new_datum.tokens) \n",
    "\t\n",
    "\t\t# Incorporating new info -> latent state, performing latent evolutions.\n",
    "\t\tencode(new_datum)\n",
    "\t\tevolve_latent()\n",
    "\n",
    "\t\t# returning values \n",
    "\t\tif return_latent: \n",
    "\t\t\treturn new_loss, self.latent\n",
    "\t\telse:\n",
    "\t\t\treturn new_loss\n",
    "\t\t\n",
    "\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Imports & Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 14:37:48.229837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-17 14:37:48.525672: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-17 14:37:48.647148: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-17 14:37:49.596170: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aman/miniconda3/lib/\n",
      "2022-10-17 14:37:49.596319: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aman/miniconda3/lib/\n",
      "2022-10-17 14:37:49.596331: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "## Import Box \n",
    "import os \n",
    "import sys \n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU Setup\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting VideoSet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]2022-10-17 14:38:10.690864: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-17 14:38:11.978166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20168 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2022-10-17 14:38:11.979143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22279 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:68:00.0, compute capability: 8.6\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making patches from Videoset...\n",
      "Making the flat patch set...\n",
      "Adding codes to the PatchSet...\n",
      "Flattening the coded + patched dataset...\n"
     ]
    }
   ],
   "source": [
    "## Get some data \n",
    "# Utility imports  \n",
    "sys.path.append(\"../src\")\n",
    "import video_loader as vl\n",
    "import video_preprocess as vp \n",
    "\n",
    "## Meta/constants \n",
    "DATA_FOLDER = \"../datasets/downloads\"\n",
    "num_videos, num_frames = 16, 20\n",
    "output_size = (120, 180)\n",
    "\n",
    "patch_height = 16\n",
    "patch_width = 16\n",
    "patch_duration = 3\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "# Fourier feature codes \n",
    "k_space = 15\n",
    "mu_space = 20 \n",
    "k_time = 64 \n",
    "mu_time = 200\n",
    "\n",
    "print(\"Getting VideoSet...\")\n",
    "VideoSet = vl.get_videoset(\"../datasets/downloads\", num_videos, num_frames, output_size=output_size)\n",
    "\n",
    "print(\"Making patches from Videoset...\")\n",
    "PatchSet = vp.make_patchset(VideoSet, patch_duration, patch_height, patch_width)\n",
    "\n",
    "print(\"Making the flat patch set...\")\n",
    "FlatPatchSet = vp.patch_to_flatpatch(PatchSet, batch_size=batch_size)\n",
    "\n",
    "print(\"Adding codes to the PatchSet...\")\n",
    "CodedPatchedSet = PatchSet.map(lambda x: vp.add_spacetime_codes(x, \n",
    "\t\tk_space=k_space, mu_space=mu_space, k_time=k_time, mu_time=mu_time))\n",
    "\n",
    "print(\"Flattening the coded + patched dataset...\")\n",
    "FlatCodedPatchedSet = vp.patch_to_flatpatch(CodedPatchedSet, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMF",
   "language": "python",
   "name": "mmf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb424496d90d9eff7a93e13c98d7ddc1d5da3a5925d09c92a85f3c2f93884c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
