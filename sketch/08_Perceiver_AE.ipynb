{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceiver Autoencoder\n",
    "\n",
    "In this notebook, I prototype the **full** version of the Perceiver autoencoder. This includes the training loop and any data management that needs to occur with this. Disk-memory-GPU optimization of data flow will not be performed, however. \n",
    "\n",
    "**Goals:**\n",
    "Fully adjustable + parameterizable Perceiver for Predictive Coding.\n",
    " - **Encoder**: Adjustable encoder shape, # re-exposures as data is compressed. \n",
    "\t - Query = current latent state; key-values = input byte array. \n",
    "\t \t - Re-exposures: query = current latent state estimate; key-values = input byte array.\n",
    "\t\t - **Option**: residual connections around TF blocks.  \n",
    "\t\t - **Option**: use different block types for each block after the first. \n",
    "\t - Let's avoid having a bunch of intermediate {#token, token dim} sizes between the original byte array and the final latent state. \n",
    "\t - The only adjustment should be the # re-exposures -- i.e., the number of different $\\mathbb{R}^{(M \\times D)} \\to \\mathbb{R}^{(N\\times C)}$ encoders there are that re-query the byte array using the current latent estimate. \n",
    " - **Latent-latent**: Number of distinct blocks, number of block repeats between new information exposure. \n",
    " - **Decoder**: Similar to encoder, queries = positional codes we want to reconstruct, key-values = latent matrix. \n",
    "\t - Avoid intermediate dimensionailties. \n",
    "\t - For repeated querying, do we use query = current reconstruction, key-value = latent matrix? \n",
    "\t \t - **Optional**: Can have residual connections, too!\n",
    "\t\t - **Optional**: use the same blocks for every step of re-exposure. \n",
    "\n",
    "\n",
    "## Pseudocode \n",
    "\n",
    "```python\n",
    "class Model: \n",
    "\tgoverning class vars: \n",
    "\t\tencoder = [several TF Blocks]\n",
    "\t\tnum_distinct_encoder = [int]\n",
    "\t\tnum_total_encoder = [int]\n",
    "\t\tresidual_encoder = [bool]\n",
    "\t\t\n",
    "\t\tlatent_evolver = [several TF Blocks]\n",
    "\t\tnum_distinct_latent = [int]\n",
    "\t\tnum_total_latent_cycles = [int] # if this is a 2-tuple: we randomly select some # of latent cycles in that range. \n",
    "\t\t\n",
    "\t\t\n",
    "\t\tdecoder = [several TF Blocks]\n",
    "\t\tnum_distinct_decoder = [int]\n",
    "\t\tnum_total_decoder = [int]\t\n",
    "\t\tresidual_decoder = [bool]\n",
    "\n",
    "\tclass state vars: \n",
    "\t\tlatent_state = [TF variable, learnable initial pos code]\n",
    "\n",
    "\tfunction encode(input_tokens, encoder_type=0): \n",
    "\t\t\"\"\" `encoder_type` is an integer. Change this to use different encoders \n",
    "\t\t(e.g., video vs. audio vs. text encoder). \n",
    "\t\t\"\"\"\n",
    "\t\t...\n",
    "\n",
    "\tfunction evolve_latent(): \n",
    "\t\t...\n",
    "\n",
    "\tfunction decode(positional_codes, decoder_type=0):\n",
    "\t\t...\n",
    "\n",
    "\n",
    "\tfunction test(new_datum): \n",
    "\t\t\"\"\" Basically just `call` but it won't incorporate the new datum into\n",
    "\t\tthe latent state. It will also add the test performance to the model's \n",
    "\t\t`test loss` records.  \n",
    "\t\t\"\"\"\n",
    "\t\t...\n",
    "\n",
    "\tfunction call(new_datum, return_latent=False, encoder_type=0):\n",
    "\t\t\"\"\" Given some new patches, we calculate the \"surprise\" \n",
    "\t\t\tthen incorporate the data into the latent state. \n",
    "\n",
    "\t\t\tWe finally return the surprise value, just for metric tracking. \n",
    "\t\t\"\"\"\n",
    "\t\t# Computing surprise\t\t\n",
    "\t\tpredicted_input = decode(new_datum.positional_codes, decoder_type=encoder_type)\n",
    "\t\tnew_loss = loss(predicted_input, new_datum.tokens) \n",
    "\t\n",
    "\t\t# Incorporating new info -> latent state, performing latent evolutions.\n",
    "\t\tencode(new_datum, encoder_type=encoder_type)\n",
    "\n",
    "\t\tevolve_latent()\n",
    "\n",
    "\t\t# returning values \n",
    "\t\tif return_latent: \n",
    "\t\t\treturn new_loss, self.latent\n",
    "\t\telse:\n",
    "\t\t\treturn new_loss\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Imports & Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 23:43:38.778390: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-19 23:43:39.048077: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-19 23:43:39.172089: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-19 23:43:40.139359: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aman/miniconda3/lib/\n",
      "2022-10-19 23:43:40.139527: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aman/miniconda3/lib/\n",
      "2022-10-19 23:43:40.139539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "## Import Box \n",
    "import os \n",
    "import sys \n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU Setup\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting VideoSet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]2022-10-19 23:43:46.525989: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-19 23:43:48.217352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2022-10-19 23:43:48.218512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22279 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:68:00.0, compute capability: 8.6\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making patches from Videoset...\n",
      "Making the flat patch set...\n",
      "Adding codes to the PatchSet...\n",
      "Flattening the coded + patched dataset...\n"
     ]
    }
   ],
   "source": [
    "## Get some data \n",
    "# Utility imports  \n",
    "sys.path.append(\"../src\")\n",
    "import video_loader as vl\n",
    "import video_preprocess as vp \n",
    "\n",
    "## Meta/constants \n",
    "DATA_FOLDER = \"../datasets/downloads\"\n",
    "num_videos, num_frames = 16, 20\n",
    "output_size = (120, 180)\n",
    "\n",
    "patch_height = 16\n",
    "patch_width = 16\n",
    "patch_duration = 3\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "# Fourier feature codes \n",
    "k_space = 15\n",
    "mu_space = 20 \n",
    "k_time = 64 \n",
    "mu_time = 200\n",
    "\n",
    "print(\"Getting VideoSet...\")\n",
    "VideoSet = vl.get_videoset(\"../datasets/downloads\", num_videos, num_frames, output_size=output_size)\n",
    "\n",
    "print(\"Making patches from Videoset...\")\n",
    "PatchSet = vp.make_patchset(VideoSet, patch_duration, patch_height, patch_width)\n",
    "\n",
    "print(\"Making the flat patch set...\")\n",
    "FlatPatchSet = vp.patch_to_flatpatch(PatchSet, batch_size=batch_size)\n",
    "\n",
    "print(\"Adding codes to the PatchSet...\")\n",
    "CodedPatchedSet = PatchSet.map(lambda x: vp.add_spacetime_codes(x, \n",
    "\t\tk_space=k_space, mu_space=mu_space, k_time=k_time, mu_time=mu_time))\n",
    "\n",
    "print(\"Flattening the coded + patched dataset...\")\n",
    "FlatCodedPatchedSet = vp.patch_to_flatpatch(CodedPatchedSet, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLayer(keras.layers.Layer):\n",
    "\tdef __init__(self, output_token_dim=None, n_heads=15, key_dim=15, mha_dropout=0.0): \n",
    "\t\t\"\"\" Transformer block. Input -> MHA -> residual, layernorm -> \n",
    "\t\t\t\tFFN -> residual (if possible). \n",
    "\n",
    "\t\tThe residual connections can only sometimes work. If the MHA step produces \n",
    "\t\ttokens of a dimensionality DIFFERENT than the dimensionality of the \n",
    "\t\tquery tokens, we can't perform the residual step. \n",
    "\n",
    "\t\tkwargs: \n",
    "\t\t\t`output_dim`: \tDimensionality of the output tokens. \n",
    "\t\t\t\t\t\t\tIf unspecified, defaults to the input token dimensionality\n",
    "\t\t\t\t\t\t\t(set during the `build()` function on first layer call). \n",
    "\t\t\t\t\t\t\tThe output will generally have the size [num_tokens, output_dim].\n",
    "\t\t\t`n_heads`:\t\tNumber of heads in the MHA layer. \n",
    "\t\t\t`key_dim`:\t\tDimensionality of keys within each MHA layer. \n",
    "\t\t\t`mha_dropout`: \tDropout rate for the multihead attention layer. \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(TFLayer, self).__init__() \n",
    "\n",
    "\t\tself.output_token_dim = output_token_dim\n",
    "\n",
    "\t\t# whether we perform the final residual; updated in `build() based on whether the output token dimensionality \n",
    "\t\tself.ffn_residual = True \n",
    "\t\tself.input_token_dim = None # set during `build()`\n",
    "\n",
    "\t\tself.MHA = keras.layers.MultiHeadAttention(num_heads=n_heads, key_dim=key_dim, dropout=mha_dropout)\n",
    "\n",
    "\t\t# We wait until `build` to construct our convolution layers.\n",
    "\n",
    "\t\t# Layernorms\n",
    "\t\tself.layer_norm_mha = tf.keras.layers.LayerNormalization()\n",
    "\t\tself.layer_norm_ffn = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "\tdef build(self, call_shapes): \n",
    "\t\tself.MHA._build_from_signature(*call_shapes)\n",
    "\t\tq_shape, k_shape = call_shapes\n",
    "\n",
    "\t\tinput_dim = q_shape[-1]\n",
    "\t\tself.input_token_dim = input_dim\n",
    "\n",
    "\t\tself.conv1 = keras.layers.Conv1D(input_dim*4, 1, activation=\"relu\")\n",
    "\n",
    "\t\t# if not specified, the output tokens will be the same size as the input token size.\n",
    "\t\tif self.output_token_dim == None:\n",
    "\t\t\tself.output_token_dim = input_dim \n",
    "\t\t\n",
    "\t\tif not (self.output_token_dim == input_dim): \n",
    "\t\t\tself.ffn_residual = False\n",
    "\n",
    "\t\tself.conv2 = keras.layers.Conv1D(self.output_token_dim, 1, activation=\"linear\") \n",
    "\n",
    "\tdef call(self, kv_list, verbose=False): \n",
    "\t\t\"\"\" Invokes the transformer block on the given queries and keys. \n",
    "\t\t\"\"\"\n",
    "\t\tqueries, keys = kv_list\n",
    "\n",
    "\t\t# Multihead attention sub-block.\n",
    "\t\tmha_out = self.MHA(queries, keys)\n",
    "\t\tmha_out = self.layer_norm_mha(mha_out + queries) # residual + layernorm\n",
    "\n",
    "\t\t# FFN sub-block\n",
    "\t\tffn1_out = self.conv1(mha_out)\n",
    "\t\tffn2_out = self.conv2(ffn1_out)\n",
    "\t\tif self.ffn_residual:\n",
    "\t\t\tffn2_out += mha_out # performing residual if the out_dim == in_dim\n",
    "\t\treturn self.layer_norm_ffn(ffn2_out) # final layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## Testing out the TF layer, making sure everything's in order. \n",
    "tfl = TFLayer(output_token_dim=400)\n",
    "B = 10\n",
    "num_Q = 31\n",
    "dim_Q = 20\n",
    "Q_in = tf.random.uniform([B, num_Q, dim_Q])\n",
    "\n",
    "num_K = 44\n",
    "dim_K = 40\n",
    "K_in = tf.random.uniform([B, num_K, dim_K])\n",
    "\n",
    "output = tfl([Q_in, K_in])\n",
    "print(output.shape == [B, num_Q, 400])\n",
    "\n",
    "tfl = TFLayer(output_token_dim=None)\n",
    "output = tfl([Q_in, K_in])\n",
    "print(output.shape == Q_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverAE(keras.Model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMF",
   "language": "python",
   "name": "mmf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb424496d90d9eff7a93e13c98d7ddc1d5da3a5925d09c92a85f3c2f93884c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
