{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceiver Autoencoder\n",
    "\n",
    "In this notebook, I prototype the **full** version of the Perceiver autoencoder. This includes the training loop and any data management that needs to occur with this. Disk-memory-GPU optimization of data flow will not be performed, however. \n",
    "\n",
    "**Goals:**\n",
    "Fully adjustable + parameterizable Perceiver for Predictive Coding.\n",
    " - **Encoder**: Adjustable encoder shape, # re-exposures as data is compressed. \n",
    "\t - Query = current latent state; key-values = input byte array. \n",
    "\t \t - Re-exposures: query = current latent state estimate; key-values = input byte array.\n",
    "\t\t - **Option**: residual connections around TF blocks.  \n",
    "\t\t - **Option**: use different block types for each block after the first. \n",
    "\t - Let's avoid having a bunch of intermediate {#token, token dim} sizes between the original byte array and the final latent state. \n",
    "\t - The only adjustment should be the # re-exposures -- i.e., the number of different $\\mathbb{R}^{(M \\times D)} \\to \\mathbb{R}^{(N\\times C)}$ encoders there are that re-query the byte array using the current latent estimate. \n",
    " - **Latent-latent**: Number of distinct blocks, number of block repeats between new information exposure. \n",
    " - **Decoder**: Similar to encoder, queries = positional codes we want to reconstruct, key-values = latent matrix. \n",
    "\t - Avoid intermediate dimensionailties. \n",
    "\t - For repeated querying, do we use query = current reconstruction, key-value = latent matrix? \n",
    "\t \t - **Optional**: Can have residual connections, too!\n",
    "\t\t - **Optional**: use the same blocks for every step of re-exposure. \n",
    "\n",
    "\n",
    "## Pseudocode \n",
    "\n",
    "```python\n",
    "class Model: \n",
    "\tgoverning class vars: \n",
    "\t\tencoder = [several TF Blocks]\n",
    "\t\tnum_distinct_encoder = [int]\n",
    "\t\tnum_total_encoder = [int]\n",
    "\t\tresidual_encoder = [bool]\n",
    "\t\t\n",
    "\t\tlatent_evolver = [several TF Blocks]\n",
    "\t\tnum_distinct_latent = [int]\n",
    "\t\tnum_total_latent_cycles = [int] # if this is a 2-tuple: we randomly select some # of latent cycles in that range. \n",
    "\t\t\n",
    "\t\t\n",
    "\t\tdecoder = [several TF Blocks]\n",
    "\t\tnum_distinct_decoder = [int]\n",
    "\t\tnum_total_decoder = [int]\t\n",
    "\t\tresidual_decoder = [bool]\n",
    "\n",
    "\tclass state vars: \n",
    "\t\tlatent_state = [TF variable, learnable initial pos code]\n",
    "\n",
    "\tfunction encode(input_tokens, encoder_type=0): \n",
    "\t\t\"\"\" `encoder_type` is an integer. Change this to use different encoders \n",
    "\t\t(e.g., video vs. audio vs. text encoder). \n",
    "\t\t\"\"\"\n",
    "\t\t...\n",
    "\n",
    "\tfunction evolve_latent(): \n",
    "\t\t...\n",
    "\n",
    "\tfunction decode(positional_codes, decoder_type=0):\n",
    "\t\t...\n",
    "\n",
    "\n",
    "\tfunction test(new_datum): \n",
    "\t\t\"\"\" Basically just `call` but it won't incorporate the new datum into\n",
    "\t\tthe latent state. It will also add the test performance to the model's \n",
    "\t\t`test loss` records.  \n",
    "\t\t\"\"\"\n",
    "\t\t...\n",
    "\n",
    "\tfunction call(new_datum, return_latent=False, encoder_type=0):\n",
    "\t\t\"\"\" Given some new patches, we calculate the \"surprise\" \n",
    "\t\t\tthen incorporate the data into the latent state. \n",
    "\n",
    "\t\t\tWe finally return the surprise value, just for metric tracking. \n",
    "\t\t\"\"\"\n",
    "\t\t# Computing surprise\t\t\n",
    "\t\tpredicted_input = decode(new_datum.positional_codes, decoder_type=encoder_type)\n",
    "\t\tnew_loss = loss(predicted_input, new_datum.tokens) \n",
    "\t\n",
    "\t\t# Incorporating new info -> latent state, performing latent evolutions.\n",
    "\t\tencode(new_datum, encoder_type=encoder_type)\n",
    "\n",
    "\t\tevolve_latent()\n",
    "\n",
    "\t\t# returning values \n",
    "\t\tif return_latent: \n",
    "\t\t\treturn new_loss, self.latent\n",
    "\t\telse:\n",
    "\t\t\treturn new_loss\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Imports & Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 23:43:38.778390: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-19 23:43:39.048077: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-19 23:43:39.172089: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-19 23:43:40.139359: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aman/miniconda3/lib/\n",
      "2022-10-19 23:43:40.139527: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aman/miniconda3/lib/\n",
      "2022-10-19 23:43:40.139539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "## Import Box \n",
    "import os \n",
    "import sys \n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU Setup\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting VideoSet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]2022-10-19 23:43:46.525989: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-19 23:43:48.217352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2022-10-19 23:43:48.218512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22279 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:68:00.0, compute capability: 8.6\n",
      "100%|██████████| 16/16 [00:03<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making patches from Videoset...\n",
      "Making the flat patch set...\n",
      "Adding codes to the PatchSet...\n",
      "Flattening the coded + patched dataset...\n"
     ]
    }
   ],
   "source": [
    "## Get some data \n",
    "# Utility imports  \n",
    "sys.path.append(\"../src\")\n",
    "import video_loader as vl\n",
    "import video_preprocess as vp \n",
    "\n",
    "## Meta/constants \n",
    "DATA_FOLDER = \"../datasets/downloads\"\n",
    "num_videos, num_frames = 16, 20\n",
    "output_size = (120, 180)\n",
    "\n",
    "patch_height = 16\n",
    "patch_width = 16\n",
    "patch_duration = 3\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "# Fourier feature codes \n",
    "k_space = 15\n",
    "mu_space = 20 \n",
    "k_time = 64 \n",
    "mu_time = 200\n",
    "\n",
    "print(\"Getting VideoSet...\")\n",
    "VideoSet = vl.get_videoset(\"../datasets/downloads\", num_videos, num_frames, output_size=output_size)\n",
    "\n",
    "print(\"Making patches from Videoset...\")\n",
    "PatchSet = vp.make_patchset(VideoSet, patch_duration, patch_height, patch_width)\n",
    "\n",
    "print(\"Making the flat patch set...\")\n",
    "FlatPatchSet = vp.patch_to_flatpatch(PatchSet, batch_size=batch_size)\n",
    "\n",
    "print(\"Adding codes to the PatchSet...\")\n",
    "CodedPatchedSet = PatchSet.map(lambda x: vp.add_spacetime_codes(x, \n",
    "\t\tk_space=k_space, mu_space=mu_space, k_time=k_time, mu_time=mu_time))\n",
    "\n",
    "print(\"Flattening the coded + patched dataset...\")\n",
    "FlatCodedPatchedSet = vp.patch_to_flatpatch(CodedPatchedSet, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Model Definition\n",
    "\n",
    "To keep the model as modular as possible, I will define custom layers for the \n",
    "`encoder`, `decoder`, and `latent_evolver` blocks. While it may seem \n",
    "inconvenient due to unfamiliarity with the keras Layers customization API, \n",
    "it should help by facilitating debugging, future weight exchanges between \n",
    "models, and more. \n",
    "\n",
    "I will also document any seemingly novel design decisions here. Hopefully this \n",
    "will be helpful during writing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a: Transformer Layers\n",
    "\n",
    "Before considering encoder/decoder/latent evolver blocks, we need to define a \n",
    "good Transformer block abstraction. Recall the operations of a transformer \n",
    "block: Given input `query_in, key_in`, \n",
    " 1. `mha_out` is calculated. The query is added as a residual connection, and a \n",
    "\t`layernorm` is applied. \n",
    " 2. `feedforward_net` is applied to the MHA's output. Residual connection is \n",
    "\tadded around the feedforward network and a `layernorm` is applied again.  \n",
    "\n",
    "Notably, we are interested in reprojecting the tokens into vastly different \n",
    "sizes. The number of input tokens and their dimensionalities may not always \n",
    "match the number of output tokens and their dimensionalities. \n",
    "\n",
    "This introduces an important question: If we desire a different output dimensionality or \n",
    "number of tokens, where and how do we do this within a transformer layer? \n",
    "\n",
    "__Token dimensionality changes__: This can be accomplished in a very\n",
    "straight-forward manner by changing the output dimension of the\n",
    "`feedforward_net`.  However, this means that the output token dimensionality\n",
    "won't match the intermediate token dimensionality that came out of the MHA\n",
    "layer! The residual connection around the `feedforward_net` is no longer\n",
    "possible. \n",
    "\n",
    "We could move the problem back one block by changing the `output_dim` of the MHA\n",
    "layer (i.e., output values are calculated through a dimension-expanding\n",
    "projection of the input keys). This is doesn't solve much, though, because \n",
    "now the residual connection around the MHA layer is no longer possible!\n",
    "\n",
    "Overall, it's simpler and cleaner to just change the output dimensionality. \n",
    "There's probably some interesting theory you could do about this question, but \n",
    "the point is that it preserves the most about the original transformer block \n",
    "design AND it's really easy to implement. \n",
    "\n",
    "__Number of token changes__: To force the number of tokens in the output to \n",
    "NOT match the number of tokens in the input, you need to use a `query_in` that \n",
    "has the desired number of tokens. \n",
    "\n",
    "The queries can be thought to ask, \"what value should I take on next?\". They \n",
    "dictate the number of tokens in the output, and usually the output also matches \n",
    "their dimensionality. Meanwhile, keys can be thought to \"give information\" in \n",
    "order to inform the update to the queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLayer(keras.layers.Layer):\n",
    "\tdef __init__(self, output_token_dim=None, n_heads=15, key_dim=15, mha_dropout=0.0): \n",
    "\t\t\"\"\" Transformer block. Input -> MHA -> residual, layernorm -> \n",
    "\t\t\t\tFFN -> residual (if possible). \n",
    "\n",
    "\t\tThe residual connections can only sometimes work. If the MHA step produces \n",
    "\t\ttokens of a dimensionality DIFFERENT than the dimensionality of the \n",
    "\t\tquery tokens, we can't perform the residual step. \n",
    "\n",
    "\t\tkwargs: \n",
    "\t\t\t`output_dim`: \tDimensionality of the output tokens. \n",
    "\t\t\t\t\t\t\tIf unspecified, defaults to the input token dimensionality\n",
    "\t\t\t\t\t\t\t(set during the `build()` function on first layer call). \n",
    "\t\t\t\t\t\t\tThe output will generally have the size [num_tokens, output_dim].\n",
    "\t\t\t`n_heads`:\t\tNumber of heads in the MHA layer. \n",
    "\t\t\t`key_dim`:\t\tDimensionality of keys within each MHA layer. \n",
    "\t\t\t`mha_dropout`: \tDropout rate for the multihead attention layer. \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(TFLayer, self).__init__() \n",
    "\n",
    "\t\tself.output_token_dim = output_token_dim\n",
    "\n",
    "\t\t# whether we perform the final residual; updated in `build() based on whether the output token dimensionality \n",
    "\t\tself.ffn_residual = True \n",
    "\t\tself.input_token_dim = None # set during `build()`\n",
    "\n",
    "\t\tself.MHA = keras.layers.MultiHeadAttention(num_heads=n_heads, key_dim=key_dim, dropout=mha_dropout)\n",
    "\n",
    "\t\t# We wait until `build` to construct our convolution layers.\n",
    "\n",
    "\t\t# Layernorms\n",
    "\t\tself.layer_norm_mha = tf.keras.layers.LayerNormalization()\n",
    "\t\tself.layer_norm_ffn = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "\tdef build(self, call_shapes): \n",
    "\t\tself.MHA._build_from_signature(*call_shapes)\n",
    "\t\tq_shape, k_shape = call_shapes\n",
    "\n",
    "\t\tinput_dim = q_shape[-1]\n",
    "\t\tself.input_token_dim = input_dim\n",
    "\n",
    "\t\tself.conv1 = keras.layers.Conv1D(input_dim*4, 1, activation=\"relu\")\n",
    "\n",
    "\t\t# if not specified, the output tokens will be the same size as the input token size.\n",
    "\t\tif self.output_token_dim == None:\n",
    "\t\t\tself.output_token_dim = input_dim \n",
    "\t\t\n",
    "\t\tif not (self.output_token_dim == input_dim): \n",
    "\t\t\tself.ffn_residual = False\n",
    "\n",
    "\t\tself.conv2 = keras.layers.Conv1D(self.output_token_dim, 1, activation=\"linear\") \n",
    "\n",
    "\tdef call(self, kv_list, verbose=False): \n",
    "\t\t\"\"\" Invokes the transformer block on the given queries and keys. \n",
    "\t\t\"\"\"\n",
    "\t\tqueries, keys = kv_list\n",
    "\n",
    "\t\t# Multihead attention sub-block.\n",
    "\t\tmha_out = self.MHA(queries, keys)\n",
    "\t\tmha_out = self.layer_norm_mha(mha_out + queries) # residual + layernorm\n",
    "\n",
    "\t\t# FFN sub-block\n",
    "\t\tffn1_out = self.conv1(mha_out)\n",
    "\t\tffn2_out = self.conv2(ffn1_out)\n",
    "\t\tif self.ffn_residual:\n",
    "\t\t\tffn2_out += mha_out # performing residual if the out_dim == in_dim\n",
    "\t\treturn self.layer_norm_ffn(ffn2_out) # final layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## Testing out the TF layer, making sure everything's in order. \n",
    "tfl = TFLayer(output_token_dim=400)\n",
    "B = 10\n",
    "num_Q = 31\n",
    "dim_Q = 20\n",
    "Q_in = tf.random.uniform([B, num_Q, dim_Q])\n",
    "\n",
    "num_K = 44\n",
    "dim_K = 40\n",
    "K_in = tf.random.uniform([B, num_K, dim_K])\n",
    "\n",
    "output = tfl([Q_in, K_in])\n",
    "print(output.shape == [B, num_Q, 400])\n",
    "\n",
    "tfl = TFLayer(output_token_dim=None)\n",
    "output = tfl([Q_in, K_in])\n",
    "print(output.shape == Q_in.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b: Encoder Layer \n",
    "\n",
    "__Operation__: The encoder layer is responsible for mapping some new high\n",
    "dimensional information (input) and the existing low-dimensionality latent space\n",
    "into an updated verison of the latent state. \n",
    "\n",
    "To accomplish this, we leverage principles from the Perceiver paper: \n",
    " 1. High-dim to low-dim transformation, then a bunch of low-dim to low-dim \n",
    " transformations to perform efficient processing. \n",
    " 2. \"Re-exposure\" of the low-dimensional representation to the high-dim input \n",
    " by assigning `query` := low-dim representation, `key-values` := high-dim \n",
    " input. \n",
    "\n",
    "We are also interested in investigation the applicability of the following\n",
    "ideas: \n",
    " 1. Prediction error/loss based on incoming new high-dim information (cf.\n",
    " \"surprise\" in Friston's free energy). \n",
    " 2. Random selection of tokens from input data to *actually* expose to the\n",
    " model.\n",
    " 3. Random \"wait times\" (in terms of latent cycles) between subjecting the model\n",
    " to new information/re-exposing to previously seen information. \n",
    " 4. Multiple modalities of information being input AND predicted, randomly \n",
    " rotated between (each has its own decoder but feeds into the same latent space).\n",
    "\n",
    "\n",
    "__Where does initial `latent` query value come from?__ Overall, it's going to be \n",
    "far easier to have the top-level `PerceiverAE` model supply this information. \n",
    "The alternative involves coordinating memory clearing/altering operations \n",
    "between layers, which seems hard to keep track of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X, Y, msk tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]], shape=(3, 2), dtype=float32) tf.Tensor(\n",
      "[[3. 4.]\n",
      " [5. 6.]], shape=(2, 2), dtype=float32) tf.Tensor([False  True  True], shape=(3,), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# Test: how to randomly sample from batches of data? \n",
    "\n",
    "X = tf.constant([[1, 2], [3, 4], [5, 6]], dtype=tf.float32)\n",
    "selector = tf.random.uniform([3])\n",
    "msk = selector > 0.5\n",
    "Y = tf.boolean_mask(X, msk, axis=0)\n",
    "print(\"X, Y, msk\", X, Y, msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAE_Encoder(keras.layers.Layer):\n",
    "\tdef __init__(self, n_blocks, p_droptoken=0.5, re_droptoken=True, \n",
    "\t\t\ttfblock_residual=True, n_heads=15, key_dim=15, mha_dropout=0.0): \n",
    "\t\t\"\"\" \t\t\n",
    "\t\tPerceiver AE encoder layer. Queries during the call should be the \n",
    "\t\tcurrent `latent` state, and the key-values should be the incoming \n",
    "\t\tbyte array.\n",
    "\n",
    "\t\tThe encoder sequentially re-queries the incoming byte array using the \n",
    "\t\tlatent state. The latent state used for re-querying is summed with \n",
    "\t\tthe result of each block, then the layernorm is taken (i.e., residual \n",
    "\t\t+ layernorm). \n",
    "\n",
    "\t\targs: \n",
    "\t\t\t`n_heads`, `key_dim`, `mha_dropout`: Params uniformly applied to all transformer \n",
    "\t\t\t\t\t\t\tblocks. \n",
    "\t\t\t`n_blocks`: \tNumber transformer blocks performing successive \n",
    "\t\t\t\t\t\t\trequerying of the input. \n",
    "\t\t\t`p_droptoken`: \tPortion of tokens to MAINTAIN from the input. \n",
    "\t\t\t`re_droptoken`: Do we reselect the dropped tokens every time we \n",
    "\t\t\t\t\t\t\tsuccessively re-query the input? \n",
    "\t\tkwargs: \n",
    "\t\t\t`output_dim`: \tDimensionality of the output tokens. \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(PAE_Encoder, self).__init__() \n",
    "\n",
    "\t\t# Recording parameters. \n",
    "\t\tself.n_blocks = n_blocks\n",
    "\t\tself.p_droptoken = p_droptoken\n",
    "\t\tself.re_droptoken = re_droptoken\n",
    "\t\tself.tfblock_residual = tfblock_residual\n",
    "\n",
    "\t\tself.n_heads = n_heads\n",
    "\t\tself.key_dim = key_dim\n",
    "\t\tself.mha_dropout = mha_dropout\n",
    "\n",
    "\t\t# Component layers\n",
    "\t\tself.tf_layers\t = [TFLayer(n_heads=self.n_heads, key_dim=self.key_dim, mha_dropout=self.mha_dropout) for i in range(self.n_blocks)]\n",
    "\t\tself.layer_norms = [tf.keras.layers.LayerNormalization() for i in range(self.n_blocks)]\n",
    "\n",
    "\n",
    "\tdef call(self, kv_list, no_drop=False,  verbose=False): \n",
    "\t\t\"\"\" Invokes the encoder module on a latent state and an input array. \n",
    "\n",
    "\t\targs: \n",
    "\t\t\t`kv_list`: \t\tTuple or list [current_latent, input_array]\n",
    "\t\t\t\t\t\t\tcurrent_latent.shape = [batch_size, N, C]\n",
    "\t\t\t\t\t\t\tinput_array.shape    = [batch_size, M, D]\n",
    "\t\t\"\"\"\n",
    "\t\tlatent, input_byte_array = kv_list\n",
    "\n",
    "\t\t# Length = # tokens in input\n",
    "\t\tif not no_drop:\n",
    "\t\t\tdroptoken_mask = tf.random.uniform([input_byte_array.shape[1]]) < self.p_droptoken\n",
    "\t\t\tcurrent_input = tf.boolean_mask(input_byte_array, droptoken_mask, axis=1)\n",
    "\t\telse: \n",
    "\t\t\tcurrent_input = input_byte_array\n",
    "\n",
    "\t\tprint(\"Initial input size: \", input_byte_array.shape)\n",
    "\t\tprint(\"Sampled input size: \", current_input.shape)\n",
    "\n",
    "\t\t# iterating through: \n",
    "\t\tfor i in range(self.n_blocks):\n",
    "\t\t\tassert current_input.shape[1] > 0, \"Random sampling failed!\"\n",
    "\n",
    "\t\t\tlatent_ = self.tf_layers[i]([latent, current_input])\n",
    "\n",
    "\t\t\t# If we are applying tfblock_residuals: \n",
    "\t\t\tif self.tfblock_residual: \n",
    "\t\t\t\tlatent_ = self.layer_norms[i](latent_ + latent)\n",
    "\n",
    "\t\t\tlatent = latent_\n",
    "\n",
    "\t\t\t# If we are going to reselect tokens from the input for every \n",
    "\t\t\t# re-querying: \n",
    "\t\t\tif self.re_droptoken and not no_drop: \n",
    "\t\t\t\tdroptoken_mask = tf.random.uniform([input_byte_array.shape[1]]) \n",
    "\t\t\t\tcurrent_input = tf.boolean_mask(input_byte_array, droptoken_mask, axis=1)\n",
    "\n",
    "\t\treturn latent\n",
    "\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial input size:  (10, 1000, 128)\n",
      "Sampled input size:  (10, 494, 128)\n",
      "t_latent shape:  (10, 97, 64)\n",
      "t_byte_array shape:  (10, 1000, 128)\n",
      "out_latent shape:  (10, 97, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "361833"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Testing the encoder module: \n",
    "test_enc = PAE_Encoder(3) # 3 blocks\n",
    "\n",
    "B, N, C, M, D = 10, 97, 64, 1000, 128\n",
    "\n",
    "t_latent = tf.random.uniform([B, N, C])\n",
    "t_byte_array = tf.random.uniform([B, M, D])\n",
    "\n",
    "out_latent = test_enc([t_latent, t_byte_array])\n",
    "print(\"t_latent shape: \", t_latent.shape)\n",
    "print(\"t_byte_array shape: \", t_byte_array.shape)\n",
    "print(\"out_latent shape: \", out_latent.shape)\n",
    "\n",
    "# plt.imshow(out_latent[0,:,:])\n",
    "test_enc.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c: Decoder Layer \n",
    "\n",
    "Largely the same as the encoder, only we're using the `latent` state as the\n",
    "key-value information and using a Fourier spacetime code set as the `query`.\n",
    "There is only really one novel design challenge: \n",
    "\n",
    "__When to expand token dimension?__ The token dimension in the Fourier spacetime\n",
    "codes are unlikely to be the same as the source token dimensionality `M`. We can \n",
    "expand out dimension to `M` by specifying `output_token_dim` in one of the \n",
    "transformer blocks as we sequentially re-query using the spacetime codes. After \n",
    "we set `output_token_dim` to `M`, the query will have the proper dimensions to \n",
    "reconstruct the `[M, num_spacetime_codes]` patches as requested. \n",
    "\n",
    "So where do we perform this expansion? It's easy enough to add as a parameter \n",
    "for this custom layer, so let's do that. I honestly don't know where would be \n",
    "best, it probably depends on the spacetime code dimensions and the patch size...\n",
    "Doing the expansion near the end seems inviting, though, since we would have a \n",
    "nice lower-dimensional patch representation right before that expansion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAE_Decoder(keras.layers.Layer):\n",
    "\tdef __init__(self, output_patch_dim, n_blocks, expansion_block_num, \n",
    "\t\t\ttfblock_residual=True, n_heads=15, key_dim=15, mha_dropout=0.0): \n",
    "\t\t\"\"\" Perceiver AE decoder layer. This layer maps from a pair of tensors: \n",
    "\t\t\tquery\t\t= spacetime codes of the patches we want to reconstruct, \n",
    "\t\t\tkey-values\t= current latent tensor.\n",
    "\n",
    "\t\tMuch like the encoder layer, several transformer blocks are used to \n",
    "\t\tsuccessively re-query the key-value tensor (in this case, the latent \n",
    "\t\ttensor from the model). It's like making \"successive approximations\" of \n",
    "\t\tthe reconstruction. \n",
    "\n",
    "\t\tThe query is the spacetime codes of the patches we want to reconstruct.\n",
    "\t\tThis query is iteratively transformed into the reconstruction itself. \n",
    "\t\t\n",
    "\n",
    "\t\targs: \n",
    "\t\t\t`output_patch_dim`: Dimensionality of the patches we want to \n",
    "\t\t\t\t\t\treconstruct. \n",
    "\t\t\t`n_blocks`: Number transformer blocks performing successive \n",
    "\t\t\t\t\t\trequerying of the input. \n",
    "\n",
    "\t\t\t`expansion_block_num`: On which of the `n_blocks` blocks do we make \n",
    "\t\t\t\t\t\tthe transition (on the query) from the token\n",
    "\t\t\t\t\t\tdimensionality of the spacetime codes to the token\n",
    "\t\t\t\t\t\tdimensionality of the patches we want to reconstruct?\n",
    "\t\t\t\n",
    "\t\tkwargs: \n",
    "\t\t\t`tfblock_residual`: Do we include residual connections around the \n",
    "\t\t\t\t\t\ttransformer blocks? \n",
    "\n",
    "\t\t\t`n_heads`, `key_dim`, `mha_dropout`: Params uniformly applied to all \n",
    "\t\t\t\t\t\ttransformer blocks. \n",
    "\n",
    "\t\t\t`output_dim`: \tDimensionality of the output tokens. \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(PAE_Decoder, self).__init__() \n",
    "\n",
    "\t\t# Recording parameters. \n",
    "\t\tself.output_patch_dim = output_patch_dim\n",
    "\t\tself.n_blocks = n_blocks\n",
    "\t\tself.expansion_block_num = expansion_block_num\n",
    "\t\tself.tfblock_residual = tfblock_residual\n",
    "\n",
    "\t\tself.n_heads = n_heads\n",
    "\t\tself.key_dim = key_dim\n",
    "\t\tself.mha_dropout = mha_dropout\n",
    "\n",
    "\t\t# Component layers\n",
    "\t\tself.tf_layers\t = [TFLayer(n_heads=self.n_heads, key_dim=self.key_dim, mha_dropout=self.mha_dropout) for i in range(self.n_blocks)]\n",
    "\t\tself.tf_layers[self.expansion_block_num] = TFLayer(output_token_dim=self.output_patch_dim, n_heads=self.n_heads, key_dim=self.key_dim, mha_dropout=self.mha_dropout)\n",
    "\t\tself.layer_norms = [tf.keras.layers.LayerNormalization() for i in range(self.n_blocks)]\n",
    "\n",
    "\n",
    "\tdef call(self, kv_list, verbose=False): \n",
    "\t\t\"\"\" Invokes the encoder module on a latent state and an input array. \n",
    "\n",
    "\t\targs: \n",
    "\t\t\t`kv_list`: \t\tTuple or list [spacetime_codes, latents]\n",
    "\t\t\t\t\t\t\tcurrent_latent.shape = [batch_size, N, C]\n",
    "\t\t\t\t\t\t\tinput_array.shape    = [batch_size, M, D]\n",
    "\t\t\"\"\"\n",
    "\t\tquery_spacetime_code, latents = kv_list\n",
    "\n",
    "\t\treconstruction = query_spacetime_code \n",
    "\n",
    "\t\t# iterating through: \n",
    "\t\tfor i in range(self.n_blocks):\n",
    "\t\t\treconstruction_ = self.tf_layers[i]([reconstruction, latents])\n",
    "\n",
    "\t\t\t# If we are applying tfblock_residuals: \n",
    "\t\t\tif self.tfblock_residual and i != self.expansion_block_num: \n",
    "\t\t\t\treconstruction_ = self.layer_norms[i](reconstruction_ + reconstruction)\n",
    "\n",
    "\t\t\treconstruction = reconstruction_ \n",
    "\n",
    "\t\treturn reconstruction\n",
    "\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_latent shape:  (10, 97, 64)\n",
      "t_spacetime_query shape:  (10, 1000, 128)\n",
      "reconstruction shape:  (10, 1000, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2973487"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Testing the decoder layer \n",
    "out_patch_dim = 256\n",
    "n_blocks = 5 \n",
    "expansion_block_num = 0 # 0-indexed!\n",
    "test_decoder = PAE_Decoder(out_patch_dim, n_blocks, expansion_block_num) \n",
    "\n",
    "B, N, C, M, D = 10, 97, 64, 1000, 128\n",
    "\n",
    "t_latent = tf.random.uniform([B, N, C])\n",
    "t_spacetime_query = tf.random.uniform([B, M, D])\n",
    "\n",
    "reconstruction = test_decoder([t_spacetime_query, t_latent])\n",
    "print(\"t_latent shape: \", t_latent.shape)\n",
    "print(\"t_spacetime_query shape: \", t_spacetime_query.shape)\n",
    "print(\"reconstruction shape: \", reconstruction.shape)\n",
    "\n",
    "# plt.imshow(out_latent[0,:,:])\n",
    "test_decoder.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d: Latent-latent layer \n",
    "\n",
    "__Operation:__ A sequence of self-attention layers is applied to the latent\n",
    "state. \n",
    " - These layers can share weights or can have different weights. \n",
    " - The number of latent-latent evolution cycles is controlled by the top-level\n",
    "   perceiver autoencoder.\n",
    " - `Option`: Residual connections around the full transformer blocks? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAE_Latent_Evolver(keras.layers.Layer):\n",
    "\tdef __init__(self, n_blocks, distinct_blocks=True, \n",
    "\t\t\ttfblock_residual=True, n_heads=15, key_dim=15, mha_dropout=0.0): \n",
    "\t\t\"\"\" Perceiver AE latent evolver layer. \n",
    "\n",
    "\n",
    "\t\targs: \n",
    "\t\t\t`n_blocks`: \tTotal number of transformer blocks the data goes \n",
    "\t\t\t\t\t\t\tthrough before being returned. NOT necessarily \n",
    "\t\t\t\t\t\t\tdistinct weights!\n",
    "\t\t\t\n",
    "\t\tkwargs: \n",
    "\t\t\t`distrinct_blocks`: Should all the blocks share weights?\n",
    "\n",
    "\t\t\t`tfblock_residual`: Do we include residual connections around the \n",
    "\t\t\t\t\t\ttransformer blocks? \n",
    "\n",
    "\t\t\t`n_heads`, `key_dim`, `mha_dropout`: Params uniformly applied to all \n",
    "\t\t\t\t\t\ttransformer blocks. \n",
    "\t\t\"\"\"\n",
    "\t\tsuper(PAE_Latent_Evolver, self).__init__() \n",
    "\n",
    "\t\t# Recording parameters\n",
    "\t\tself.n_blocks \t\t = n_blocks\n",
    "\t\tself.distinct_blocks = distinct_blocks\n",
    "\t\tself.tfblock_residual = tfblock_residual\n",
    "\n",
    "\t\tself.n_heads = n_heads\n",
    "\t\tself.key_dim = key_dim\n",
    "\t\tself.mha_dropout = mha_dropout\n",
    "\n",
    "\t\t# Creating the layers \n",
    "\t\tif self.distinct_blocks: \n",
    "\t\t\tself.tf_layers = [TFLayer(n_heads=self.n_heads, key_dim=self.key_dim, mha_dropout=self.mha_dropout) for i in range(self.n_blocks)]\n",
    "\t\telse: \n",
    "\t\t\tself.tf_layers = [TFLayer(n_heads=self.n_heads, key_dim=self.key_dim, mha_dropout=self.mha_dropout)]\n",
    "\n",
    "\t\tself.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "\tdef call(self, latent_in):\n",
    "\t\tlatent = latent_in\n",
    "\n",
    "\t\tfor i in range(self.n_blocks):\n",
    "\t\t\tj = i if self.distinct_blocks else 0 \n",
    "\n",
    "\t\t\tlatent_ = self.tf_layers[j]([latent, latent])\n",
    "\n",
    "\t\t\tif self.tfblock_residual: \n",
    "\t\t\t\tlatent_ = latent + latent_\n",
    "\t\t\t\tlatent_ = self.layer_norm(latent_)\n",
    "\t\t\t\n",
    "\t\t\tlatent = latent_\n",
    "\n",
    "\t\treturn latent\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"pae__latent__evolver\" \"                 f\"(type PAE_Latent_Evolver).\n\ntoo many values to unpack (expected 2)\n\nCall arguments received by layer \"pae__latent__evolver\" \"                 f\"(type PAE_Latent_Evolver):\n  • latent_in=tf.Tensor(shape=(10, 97, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [127], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m B, N, C, M, D \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, \u001b[39m97\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m1000\u001b[39m, \u001b[39m128\u001b[39m\n\u001b[1;32m     11\u001b[0m t_latent_0 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform([B, N, C])\n\u001b[0;32m---> 13\u001b[0m t_latent_1 \u001b[39m=\u001b[39m test_ev(t_latent_0)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mt_latent_0 shape: \u001b[39m\u001b[39m\"\u001b[39m, t_latent_0\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mt_latent_1 shape: \u001b[39m\u001b[39m\"\u001b[39m, t_latent_1\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn [126], line 46\u001b[0m, in \u001b[0;36mPAE_Latent_Evolver.call\u001b[0;34m(self, latent_in)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_blocks):\n\u001b[1;32m     44\u001b[0m \tj \u001b[39m=\u001b[39m i \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistinct_blocks \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m \n\u001b[0;32m---> 46\u001b[0m \tlatent_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtf_layers[j](latent, latent)\n\u001b[1;32m     48\u001b[0m \t\u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtfblock_residual: \n\u001b[1;32m     49\u001b[0m \t\tlatent_ \u001b[39m=\u001b[39m latent \u001b[39m+\u001b[39m latent_\n",
      "Cell \u001b[0;32mIn [83], line 37\u001b[0m, in \u001b[0;36mTFLayer.build\u001b[0;34m(self, call_shapes)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, call_shapes): \n\u001b[1;32m     36\u001b[0m \t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMHA\u001b[39m.\u001b[39m_build_from_signature(\u001b[39m*\u001b[39mcall_shapes)\n\u001b[0;32m---> 37\u001b[0m \tq_shape, k_shape \u001b[39m=\u001b[39m call_shapes\n\u001b[1;32m     39\u001b[0m \tinput_dim \u001b[39m=\u001b[39m q_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     40\u001b[0m \t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_token_dim \u001b[39m=\u001b[39m input_dim\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"pae__latent__evolver\" \"                 f\"(type PAE_Latent_Evolver).\n\ntoo many values to unpack (expected 2)\n\nCall arguments received by layer \"pae__latent__evolver\" \"                 f\"(type PAE_Latent_Evolver):\n  • latent_in=tf.Tensor(shape=(10, 97, 64), dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Testing PAE latent-latent evolver\n",
    "n_blocks = 5 \n",
    "distinct_blocks = False\n",
    "# n_blocks, distinct_blocks=True, \n",
    "# \t\t\ttfblock_residual=True, n_heads=15, key_dim=15, mha_dropout=0.0): \n",
    "\n",
    "test_ev = PAE_Latent_Evolver(n_blocks, distinct_blocks=distinct_blocks) \n",
    "\n",
    "B, N, C, M, D = 10, 97, 64, 1000, 128\n",
    "\n",
    "t_latent_0 = tf.random.uniform([B, N, C])\n",
    "\n",
    "t_latent_1 = test_ev(t_latent_0)\n",
    "print(\"t_latent_0 shape: \", t_latent_0.shape)\n",
    "print(\"t_latent_1 shape: \", t_latent_1.shape)\n",
    "\n",
    "# plt.imshow(out_latent[0,:,:])\n",
    "test_ev.count_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceiverAE(keras.Model):\n",
    "\tdef __init__(self, latent_dims=(90, 77)): \n",
    "\t\tself.N, self.C = latent_dims  \t# N rows (tokens), each with dimensionality C.\n",
    "\n",
    "\t\t# Each of these \n",
    "\t\tself.encoder = self.generate_encoder() \n",
    "\t\tself.latent_evolver = self.generate_latent_ev()\n",
    "\n",
    "\n",
    "\tdef reset_latent(self, B=1): \n",
    "\t\t...\n",
    "\n",
    "\tdef call(self, reconstruct_me):\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMF",
   "language": "python",
   "name": "mmf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb424496d90d9eff7a93e13c98d7ddc1d5da3a5925d09c92a85f3c2f93884c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
