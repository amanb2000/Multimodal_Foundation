{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Preliminaries \n",
    "\n",
    "\n",
    "## Framing\n",
    "> GOAL: Create a notebook with proof-of-concepts/examples for each component of the upcoming transformer/Perceiver architecture. \n",
    "\n",
    "Objectives: \n",
    " 1. Gain familiarity with the code side of transformers.\n",
    " 2. Create examples of each unfamiliar components of the project. \n",
    " 3. Maximize copy-paste-ability. \n",
    "\n",
    "\n",
    "## Diverging \n",
    "> Things to prototype.\n",
    "\n",
    " - [ ] Patch extraction.\n",
    "\t - [ ] Video.\n",
    "\t - [ ] Audio. \n",
    "\t - [ ] VTT subtitle files. \n",
    " - [ ] Text tokenization/vectorization. \n",
    " - [ ] Implementation of spacetime Fourier features (positional encoding).\n",
    " - [ ] Attention blocks (self attention, cross attention).\n",
    "\t - Should be a `dmodel` or whatever Tensorflow uses. \n",
    " - [ ] Masking for transformer layers. \n",
    " - [ ] Decoder/encoder cold start (cf. Perceiver paper).\n",
    " - [ ] How to query latent space for specific spatiotemporal coordinates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb424496d90d9eff7a93e13c98d7ddc1d5da3a5925d09c92a85f3c2f93884c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
