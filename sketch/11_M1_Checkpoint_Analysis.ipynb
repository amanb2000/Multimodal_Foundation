{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1 Model Checkpoint Analysis\n",
    "\n",
    "Here I ensure the reproducability of results from training experiments produced \n",
    "using the scripts in `src/execute_m1.py`, `src/train_m1.py`. \n",
    " - Load model.\n",
    " - Compute performance metrics. \n",
    " - Visualize 10-frame autoencoding results. \n",
    "\n",
    "The first run of model checkpoints are in `training/2022_10_25/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 16:46:51.677371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-25 16:46:51.803006: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-25 16:46:51.828294: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-25 16:46:52.393239: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aman/miniconda3/lib/\n",
      "2022-10-25 16:46:52.393289: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/aman/miniconda3/lib/\n",
      "2022-10-25 16:46:52.393294: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "## Import Box \n",
    "import os \n",
    "import sys \n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## GPU Setup\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: Getting the Dataset\n",
    "\n",
    "_Copy-pasted from `execute_m1.py`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\t=========================\n",
      "\t=== DATASET META INFO ===\n",
      "\t=========================\n",
      "num_frames:  10\n",
      "batch_size:  10\n",
      "output_size:  (120, 180)\n",
      "Patch h/w/d:  16 16 3\n",
      "k, mu for space, time:  (15, 20) (64, 200)\n",
      "mp4_list[:10] --  ['H1iWETEMl7c.mp4', 'PmKbAgKa_V8.mp4', 'zymMiwUtbqE.mp4', 'jpeB76IUI9c.mp4', '5d-u6-UzbLk.mp4', 'm-EUUkPuocE.mp4', '_LUKHNsSndk.mp4', 'xp5sV_1SSKw.mp4', 'JXtr3GbDYHY.mp4', '3SeOIr2YisM.mp4']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t========================\n",
      "\t=== DATASET CREATION ===\n",
      "\t========================\n",
      "<FlatMapDataset element_spec=TensorSpec(shape=(1, 10, 120, 180, 3), dtype=tf.float32, name=None)>\n",
      "Making patches from Videoset...\n",
      "Making the flat patch set...\n",
      "Adding codes to the PatchSet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-25 16:49:46.018291: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-25 16:49:46.942589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17631 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2022-10-25 16:49:46.943129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22279 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:68:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattening the coded + patched dataset...\n",
      " ** Shape of FlatCodedPatchedSet element:  (10, 231, 2495)\n",
      " ** Shape of FlatCodedPatchedSet element:  (10, 231, 2495)\n",
      "Done getting datasets setup!\n"
     ]
    }
   ],
   "source": [
    "## Acquiring the dataset!\n",
    "sys.path.append(\"../src\")\n",
    "import video_loader as vl\n",
    "import video_preprocess as vp\n",
    "\n",
    "# Meta/constants -- TODO: These should be commandline arguments. \n",
    "DATA_FOLDER = \"../datasets/downloads\"\n",
    "num_frames = 10\n",
    "output_size = (120, 180)\n",
    "\n",
    "patch_height = 16\n",
    "patch_width = 16\n",
    "patch_duration = 3\n",
    "\n",
    "batch_size = 10\n",
    "num_prefetch = 4\n",
    "\n",
    "# Fourier feature codes \n",
    "k_space = 15\n",
    "mu_space = 20 \n",
    "k_time = 64 \n",
    "mu_time = 200\n",
    "\n",
    "# Mp4 list: \n",
    "mp4_list = os.listdir(DATA_FOLDER)\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"\\t=========================\")\n",
    "print(\"\\t=== DATASET META INFO ===\")\n",
    "print(\"\\t=========================\")\n",
    "print(\"num_frames: \", num_frames)\n",
    "print(\"batch_size: \", batch_size)\n",
    "print(\"output_size: \", output_size)\n",
    "print(\"Patch h/w/d: \", patch_height, patch_width, patch_duration)\n",
    "print(\"k, mu for space, time: \", (k_space, mu_space), (k_time, mu_time))\n",
    "print(\"mp4_list[:10] -- \", mp4_list[:10])\n",
    "\n",
    "\n",
    "## Creating the generator\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"\\t========================\")\n",
    "print(\"\\t=== DATASET CREATION ===\")\n",
    "print(\"\\t========================\")\n",
    "def generate_video_tensors():\n",
    "\t\"\"\" This is a generator for raw video tensors of shape \n",
    "\t[num_frames, height, width, channels]. \n",
    "\n",
    "\tIt uses global variables defined above under \"meta/constants\". These \n",
    "\twill be commandline arguments in the future. \n",
    "\t\"\"\"\n",
    "\t_mp4_list = mp4_list\n",
    "\t_DATA_FOLDER = DATA_FOLDER\n",
    "\t_output_size = output_size\n",
    "\t_num_frames = num_frames\n",
    "\twhile True: \n",
    "\t\tfor fname in _mp4_list:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tretval = vl.get_single_video_tensor(os.path.join(_DATA_FOLDER, fname), _num_frames, output_size=_output_size)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif type(retval) == np.ndarray and retval.shape[0] == _num_frames:\n",
    "\t\t\t\tyield np.expand_dims(retval, axis=0)\n",
    "\n",
    "videoset = tf.data.Dataset.from_generator(generate_video_tensors, output_signature=tf.TensorSpec(shape=[1, num_frames, *output_size, 3], dtype=tf.float32))\n",
    "print(videoset)\n",
    "\n",
    "def show_nn_sq(video_tensor, n=3, outname=\"breh.png\"):\n",
    "\tn2 = n*n\n",
    "\tfig, axs = plt.subplots(nrows=n, ncols=n, sharex=True, sharey=True)\n",
    "\n",
    "\t_nframes = video_tensor.shape[0]\n",
    "\tframe_inc = _nframes//(n2)\n",
    "\n",
    "\tfor i in range(n):\n",
    "\t\tfor j in range(n):\n",
    "\t\t\tfull_idx = i*n + j\n",
    "\n",
    "\t\t\taxs[i,j].imshow(video_tensor[frame_inc*full_idx,:,:,:])\n",
    "\t\t\taxs[i,j].set_title(f\"Frame {frame_inc*full_idx}\")\n",
    "\n",
    "\tfig.suptitle(f\"Video Survey over {_nframes} Frames\")\n",
    "\tplt.savefig(outname)\n",
    "\tplt.show()\n",
    "\n",
    "\"\"\"\n",
    "out_test = \"debug/test_from_videoset.png\"\n",
    "print(f\"Showing off an element of the videodataset(generator) in `{out_test}`\")\t\n",
    "for element in videoset:\n",
    "\tshow_nn_sq(tf.squeeze(element), outname=out_test)\n",
    "\tbreak\n",
    "\"\"\"\n",
    "\n",
    "print(\"Making patches from Videoset...\")\n",
    "PatchSet = vp.make_patchset(videoset, patch_duration, patch_height, patch_width)\n",
    "\n",
    "print(\"Making the flat patch set...\")\n",
    "FlatPatchSet = vp.patch_to_flatpatch(PatchSet, batch_size=1)\n",
    "\n",
    "print(\"Adding codes to the PatchSet...\")\n",
    "CodedPatchedSet = PatchSet.map(lambda x: vp.add_spacetime_codes(x, \n",
    "\t\tk_space=k_space, mu_space=mu_space, k_time=k_time, mu_time=mu_time))\n",
    "\n",
    "print(\"Flattening the coded + patched dataset...\")\n",
    "FlatCodedPatchedSet = vp.patch_to_flatpatch(CodedPatchedSet, batch_size=1)\n",
    "FlatCodedPatchedSet = FlatCodedPatchedSet.map(lambda x: tf.squeeze(x))\n",
    "FlatCodedPatchedSet = FlatCodedPatchedSet.batch(batch_size)\n",
    "FlatCodedPatchedSet = FlatCodedPatchedSet.prefetch(num_prefetch)\n",
    "\n",
    "cnt = 0\n",
    "for el in FlatCodedPatchedSet: \n",
    "\tprint(\" ** Shape of FlatCodedPatchedSet element: \", el.shape)\n",
    "\tcnt += 1\n",
    "\tif cnt == 2:\n",
    "\t\tbreak\n",
    "\n",
    "print(\"Done getting datasets setup!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Initializing Perceiver AE Model\n",
    "\n",
    "_Random weights, checkpoint weights will be inserted next_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t========================\n",
      "\t=== SETTING UP MODEL ===\n",
      "\t========================\n",
      "Done setting up perceiver_ae model!\n"
     ]
    }
   ],
   "source": [
    "## Setting up the model\n",
    "print(\"\\t========================\")\n",
    "print(\"\\t=== SETTING UP MODEL ===\")\n",
    "print(\"\\t========================\")\n",
    "\n",
    "import m1\n",
    "## Setting up the component modules of the top-level PAE model.\n",
    "# Encoder \n",
    "n_encoder_blocks = 3\n",
    "p_droptoken = 0.3\n",
    "re_droptoken = True\n",
    "encoder_tfres = False\n",
    "enc_nheads = 15\n",
    "enc_keydim = 15\n",
    "enc_mhadropout = 0.0\n",
    "test_encoder = m1.PAE_Encoder(n_encoder_blocks, p_droptoken=p_droptoken, re_droptoken=re_droptoken, tfblock_residual=encoder_tfres, n_heads=enc_nheads, key_dim=enc_keydim, mha_dropout=enc_mhadropout)\n",
    "\n",
    "# Latent evolver \n",
    "n_latentev_blocks = 3\n",
    "latent_distinct_blocks = False\n",
    "latent_residual = False\n",
    "latent_nheads = 15\n",
    "latent_keydim = 15\n",
    "latent_mhadropout=0.0\n",
    "test_latent_ev = m1.PAE_Latent_Evolver(n_latentev_blocks, distinct_blocks=latent_distinct_blocks, tfblock_residual=latent_residual, n_heads=latent_nheads, key_dim = latent_keydim, mha_dropout=latent_mhadropout)\n",
    "\n",
    "\n",
    "# decoder\n",
    "output_patch_dim = 2304\n",
    "n_decoder_blocks = 3\n",
    "expansion_block_num = 2\n",
    "decoder_tfres = False\n",
    "dec_nheads = 15\n",
    "dec_keydim = 15\n",
    "dec_mhadropout = 0.0\n",
    "test_decoder = m1.PAE_Decoder(output_patch_dim, n_decoder_blocks, expansion_block_num, tfblock_residual=decoder_tfres, n_heads=dec_nheads, key_dim=dec_keydim, mha_dropout=dec_mhadropout)\n",
    "\n",
    "# loss function\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "## Instantiating the PAE model!\n",
    "perceiver_ae = m1.PerceiverAE(mse, test_encoder, test_latent_ev, test_decoder, code_dim=191)\n",
    "\n",
    "perceiver_ae.reset_latent()\n",
    "\n",
    "\n",
    "print(\"Done setting up perceiver_ae model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Obtaining Checkpoint Information\n",
    "\n",
    "Question: Can we insert checkpoint weights even if we initialize with a different \n",
    "number of encoder blocks? \n",
    "\n",
    " - No error upon loading in weights!\n",
    " - No error when loading weights into model with different # latent blocks!\n",
    " - Cumulative loss over 3 \"exposures\" = 0.19 -- in line with test performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../training/2022_10_25/cp-0450.ckpt'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"../training/2022_10_25/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7ff12c2e1790>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceiver_ae.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 231, 2495])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calling the model on some data\n",
    "ele = None\n",
    "for element in FlatCodedPatchedSet:\n",
    "\tele = element\n",
    "\tbreak\n",
    "\n",
    "ele.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = perceiver_ae(ele, reset_latent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss2 = perceiver_ae(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss3 = perceiver_ae(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1:  tf.Tensor(0.11647031, shape=(), dtype=float32)\n",
      "Loss 2:  tf.Tensor(0.039892886, shape=(), dtype=float32)\n",
      "Loss 3:  tf.Tensor(0.03990355, shape=(), dtype=float32)\n",
      "\n",
      "\tSUM:  tf.Tensor(0.19626674, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss 1: \", loss1)\n",
    "print(\"Loss 2: \", loss2)\n",
    "print(\"Loss 3: \", loss3)\n",
    "print(\"\\n\\tSUM: \", loss1+loss2+loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceiver_ae.encoder.p_droptoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMF",
   "language": "python",
   "name": "mmf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb424496d90d9eff7a93e13c98d7ddc1d5da3a5925d09c92a85f3c2f93884c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
